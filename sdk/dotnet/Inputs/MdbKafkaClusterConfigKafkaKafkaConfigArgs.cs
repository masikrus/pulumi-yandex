// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Yandex.Inputs
{

    public sealed class MdbKafkaClusterConfigKafkaKafkaConfigArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Enable auto creation of topic on the server.
        /// </summary>
        [Input("autoCreateTopicsEnable")]
        public Input<bool>? AutoCreateTopicsEnable { get; set; }

        /// <summary>
        /// Compression type of kafka topics.
        /// </summary>
        [Input("compressionType")]
        public Input<string>? CompressionType { get; set; }

        /// <summary>
        /// The replication factor for automatically created topics, and for topics created with -1 as the replication factor.
        /// </summary>
        [Input("defaultReplicationFactor")]
        public Input<string>? DefaultReplicationFactor { get; set; }

        /// <summary>
        /// The number of messages accumulated on a log partition before messages are flushed to disk.
        /// </summary>
        [Input("logFlushIntervalMessages")]
        public Input<string>? LogFlushIntervalMessages { get; set; }

        /// <summary>
        /// The maximum time in ms that a message in any topic is kept in memory before flushed to disk. If not set, the value in log.flush.scheduler.interval.ms is used.
        /// </summary>
        [Input("logFlushIntervalMs")]
        public Input<string>? LogFlushIntervalMs { get; set; }

        /// <summary>
        /// The frequency in ms that the log flusher checks whether any log needs to be flushed to disk.
        /// </summary>
        [Input("logFlushSchedulerIntervalMs")]
        public Input<string>? LogFlushSchedulerIntervalMs { get; set; }

        /// <summary>
        /// Should pre allocate file when create new segment?
        /// </summary>
        [Input("logPreallocate")]
        public Input<bool>? LogPreallocate { get; set; }

        /// <summary>
        /// The maximum size of the log before deleting it.
        /// </summary>
        [Input("logRetentionBytes")]
        public Input<string>? LogRetentionBytes { get; set; }

        /// <summary>
        /// The number of hours to keep a log file before deleting it (in hours), tertiary to log.retention.ms property.
        /// </summary>
        [Input("logRetentionHours")]
        public Input<string>? LogRetentionHours { get; set; }

        /// <summary>
        /// The number of minutes to keep a log file before deleting it (in minutes), secondary to log.retention.ms property. If not set, the value in log.retention.hours is used.
        /// </summary>
        [Input("logRetentionMinutes")]
        public Input<string>? LogRetentionMinutes { get; set; }

        /// <summary>
        /// The number of milliseconds to keep a log file before deleting it (in milliseconds), If not set, the value in log.retention.minutes is used. If set to -1, no time limit is applied.
        /// </summary>
        [Input("logRetentionMs")]
        public Input<string>? LogRetentionMs { get; set; }

        /// <summary>
        /// The maximum size of a single log file.
        /// </summary>
        [Input("logSegmentBytes")]
        public Input<string>? LogSegmentBytes { get; set; }

        /// <summary>
        /// The largest record batch size allowed by Kafka (after compression if compression is enabled).
        /// </summary>
        [Input("messageMaxBytes")]
        public Input<string>? MessageMaxBytes { get; set; }

        /// <summary>
        /// The default number of log partitions per topic.
        /// </summary>
        [Input("numPartitions")]
        public Input<string>? NumPartitions { get; set; }

        /// <summary>
        /// For subscribed consumers, committed offset of a specific partition will be expired and discarded after this period of time.
        /// </summary>
        [Input("offsetsRetentionMinutes")]
        public Input<string>? OffsetsRetentionMinutes { get; set; }

        /// <summary>
        /// The number of bytes of messages to attempt to fetch for each partition.
        /// </summary>
        [Input("replicaFetchMaxBytes")]
        public Input<string>? ReplicaFetchMaxBytes { get; set; }

        [Input("saslEnabledMechanisms")]
        private InputList<string>? _saslEnabledMechanisms;

        /// <summary>
        /// The list of SASL mechanisms enabled in the Kafka server.
        /// </summary>
        public InputList<string> SaslEnabledMechanisms
        {
            get => _saslEnabledMechanisms ?? (_saslEnabledMechanisms = new InputList<string>());
            set => _saslEnabledMechanisms = value;
        }

        /// <summary>
        /// The SO_RCVBUF buffer of the socket server sockets. If the value is -1, the OS default will be used.
        /// </summary>
        [Input("socketReceiveBufferBytes")]
        public Input<string>? SocketReceiveBufferBytes { get; set; }

        /// <summary>
        /// The SO_SNDBUF buffer of the socket server sockets. If the value is -1, the OS default will be used.
        /// </summary>
        [Input("socketSendBufferBytes")]
        public Input<string>? SocketSendBufferBytes { get; set; }

        [Input("sslCipherSuites")]
        private InputList<string>? _sslCipherSuites;

        /// <summary>
        /// A list of cipher suites.
        /// </summary>
        public InputList<string> SslCipherSuites
        {
            get => _sslCipherSuites ?? (_sslCipherSuites = new InputList<string>());
            set => _sslCipherSuites = value;
        }

        public MdbKafkaClusterConfigKafkaKafkaConfigArgs()
        {
        }
        public static new MdbKafkaClusterConfigKafkaKafkaConfigArgs Empty => new MdbKafkaClusterConfigKafkaKafkaConfigArgs();
    }
}
