// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Yandex.Inputs
{

    public sealed class GetMdbClickhouseClusterUserSettingsInputArgs : global::Pulumi.ResourceArgs
    {
        /// <summary>
        /// Include CORS headers in HTTP responses.
        /// </summary>
        [Input("addHttpCorsHeader", required: true)]
        public Input<bool> AddHttpCorsHeader { get; set; } = null!;

        /// <summary>
        /// Allows or denies DDL queries.
        /// </summary>
        [Input("allowDdl", required: true)]
        public Input<bool> AllowDdl { get; set; } = null!;

        /// <summary>
        /// Enables introspections functions for query profiling.
        /// </summary>
        [Input("allowIntrospectionFunctions", required: true)]
        public Input<bool> AllowIntrospectionFunctions { get; set; } = null!;

        /// <summary>
        /// Allows specifying LowCardinality modifier for types of small fixed size (8 or less) in CREATE TABLE statements. Enabling this may increase merge times and memory consumption.
        /// </summary>
        [Input("allowSuspiciousLowCardinalityTypes", required: true)]
        public Input<bool> AllowSuspiciousLowCardinalityTypes { get; set; } = null!;

        /// <summary>
        /// Enables legacy ClickHouse server behavior in ANY INNER|LEFT JOIN operations.
        /// </summary>
        [Input("anyJoinDistinctRightTableKeys", required: true)]
        public Input<bool> AnyJoinDistinctRightTableKeys { get; set; } = null!;

        /// <summary>
        /// Enables asynchronous inserts. Disabled by default.
        /// </summary>
        [Input("asyncInsert", required: true)]
        public Input<bool> AsyncInsert { get; set; } = null!;

        /// <summary>
        /// The maximum timeout in milliseconds since the first INSERT query before inserting collected data. If the parameter is set to 0, the timeout is disabled. Default value: 200.
        /// </summary>
        [Input("asyncInsertBusyTimeout", required: true)]
        public Input<int> AsyncInsertBusyTimeout { get; set; } = null!;

        /// <summary>
        /// The maximum size of the unparsed data in bytes collected per query before being inserted. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 100000.
        /// </summary>
        [Input("asyncInsertMaxDataSize", required: true)]
        public Input<int> AsyncInsertMaxDataSize { get; set; } = null!;

        /// <summary>
        /// The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the async_insert_busy_timeout with every INSERT query as long as async_insert_max_data_size is not exceeded.
        /// </summary>
        [Input("asyncInsertStaleTimeout", required: true)]
        public Input<int> AsyncInsertStaleTimeout { get; set; } = null!;

        /// <summary>
        /// The maximum number of threads for background data parsing and insertion. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 16.
        /// </summary>
        [Input("asyncInsertThreads", required: true)]
        public Input<int> AsyncInsertThreads { get; set; } = null!;

        /// <summary>
        /// Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response. Default value: false.
        /// </summary>
        [Input("cancelHttpReadonlyQueriesOnClientClose", required: true)]
        public Input<bool> CancelHttpReadonlyQueriesOnClientClose { get; set; } = null!;

        /// <summary>
        /// Enable compilation of queries.
        /// </summary>
        [Input("compile", required: true)]
        public Input<bool> Compile { get; set; } = null!;

        /// <summary>
        /// Turn on expression compilation.
        /// </summary>
        [Input("compileExpressions", required: true)]
        public Input<bool> CompileExpressions { get; set; } = null!;

        /// <summary>
        /// Connect timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("connectTimeout", required: true)]
        public Input<int> ConnectTimeout { get; set; } = null!;

        /// <summary>
        /// The timeout in milliseconds for connecting to a remote server for a Distributed table engine, if the ‘shard’ and ‘replica’ sections are used in the cluster definition. If unsuccessful, several attempts are made to connect to various replicas. Default value: 50.
        /// </summary>
        [Input("connectTimeoutWithFailover", required: true)]
        public Input<int> ConnectTimeoutWithFailover { get; set; } = null!;

        /// <summary>
        /// Specifies which of the uniq* functions should be used to perform the COUNT(DISTINCT …) construction.
        /// </summary>
        [Input("countDistinctImplementation", required: true)]
        public Input<string> CountDistinctImplementation { get; set; } = null!;

        /// <summary>
        /// Allows choosing a parser of the text representation of date and time, one of: `best_effort`, `basic`, `best_effort_us`. Default value: `basic`. Cloud default value: `best_effort`.
        /// </summary>
        [Input("dateTimeInputFormat", required: true)]
        public Input<string> DateTimeInputFormat { get; set; } = null!;

        /// <summary>
        /// Allows choosing different output formats of the text representation of date and time, one of: `simple`, `iso`, `unix_timestamp`. Default value: `simple`.
        /// </summary>
        [Input("dateTimeOutputFormat", required: true)]
        public Input<string> DateTimeOutputFormat { get; set; } = null!;

        /// <summary>
        /// Enables or disables the deduplication check for materialized views that receive data from `Replicated` tables.
        /// </summary>
        [Input("deduplicateBlocksInDependentMaterializedViews", required: true)]
        public Input<bool> DeduplicateBlocksInDependentMaterializedViews { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow when using DISTINCT. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("distinctOverflowMode", required: true)]
        public Input<string> DistinctOverflowMode { get; set; } = null!;

        /// <summary>
        /// Determine the behavior of distributed subqueries.
        /// </summary>
        [Input("distributedAggregationMemoryEfficient", required: true)]
        public Input<bool> DistributedAggregationMemoryEfficient { get; set; } = null!;

        /// <summary>
        /// Timeout for DDL queries, in milliseconds.
        /// </summary>
        [Input("distributedDdlTaskTimeout", required: true)]
        public Input<int> DistributedDdlTaskTimeout { get; set; } = null!;

        /// <summary>
        /// Changes the behavior of distributed subqueries.
        /// </summary>
        [Input("distributedProductMode", required: true)]
        public Input<string> DistributedProductMode { get; set; } = null!;

        /// <summary>
        /// Allows to return empty result.
        /// </summary>
        [Input("emptyResultForAggregationByEmptySet", required: true)]
        public Input<bool> EmptyResultForAggregationByEmptySet { get; set; } = null!;

        /// <summary>
        /// Enables or disables data compression in the response to an HTTP request.
        /// </summary>
        [Input("enableHttpCompression", required: true)]
        public Input<bool> EnableHttpCompression { get; set; } = null!;

        /// <summary>
        /// Forces a query to an out-of-date replica if updated data is not available.
        /// </summary>
        [Input("fallbackToStaleReplicasForDistributedQueries", required: true)]
        public Input<bool> FallbackToStaleReplicasForDistributedQueries { get; set; } = null!;

        /// <summary>
        /// Sets the data format of a nested columns.
        /// </summary>
        [Input("flattenNested", required: true)]
        public Input<bool> FlattenNested { get; set; } = null!;

        /// <summary>
        /// Disables query execution if the index can’t be used by date.
        /// </summary>
        [Input("forceIndexByDate", required: true)]
        public Input<bool> ForceIndexByDate { get; set; } = null!;

        /// <summary>
        /// Disables query execution if indexing by the primary key is not possible.
        /// </summary>
        [Input("forcePrimaryKey", required: true)]
        public Input<bool> ForcePrimaryKey { get; set; } = null!;

        /// <summary>
        /// Regular expression (for Regexp format).
        /// </summary>
        [Input("formatRegexp", required: true)]
        public Input<string> FormatRegexp { get; set; } = null!;

        /// <summary>
        /// Skip lines unmatched by regular expression.
        /// </summary>
        [Input("formatRegexpSkipUnmatched", required: true)]
        public Input<bool> FormatRegexpSkipUnmatched { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow while GROUP BY operation. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// * `any` - perform approximate GROUP BY operation by continuing aggregation for the keys that got into the set, but don’t add new keys to the set.
        /// </summary>
        [Input("groupByOverflowMode", required: true)]
        public Input<string> GroupByOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets the threshold of the number of keys, after that the two-level aggregation should be used.
        /// </summary>
        [Input("groupByTwoLevelThreshold", required: true)]
        public Input<int> GroupByTwoLevelThreshold { get; set; } = null!;

        /// <summary>
        /// Sets the threshold of the number of bytes, after that the two-level aggregation should be used.
        /// </summary>
        [Input("groupByTwoLevelThresholdBytes", required: true)]
        public Input<int> GroupByTwoLevelThresholdBytes { get; set; } = null!;

        /// <summary>
        /// Connection timeout for establishing connection with replica for Hedged requests. Default value: 50 milliseconds.
        /// </summary>
        [Input("hedgedConnectionTimeoutMs", required: true)]
        public Input<int> HedgedConnectionTimeoutMs { get; set; } = null!;

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpConnectionTimeout", required: true)]
        public Input<int> HttpConnectionTimeout { get; set; } = null!;

        /// <summary>
        /// Sets minimal interval between notifications about request process in HTTP header X-ClickHouse-Progress.
        /// </summary>
        [Input("httpHeadersProgressInterval", required: true)]
        public Input<int> HttpHeadersProgressInterval { get; set; } = null!;

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpReceiveTimeout", required: true)]
        public Input<int> HttpReceiveTimeout { get; set; } = null!;

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpSendTimeout", required: true)]
        public Input<int> HttpSendTimeout { get; set; } = null!;

        /// <summary>
        /// Timeout to close idle TCP connections after specified number of seconds. Default value: 3600 seconds.
        /// </summary>
        [Input("idleConnectionTimeout", required: true)]
        public Input<int> IdleConnectionTimeout { get; set; } = null!;

        /// <summary>
        /// When performing INSERT queries, replace omitted input column values with default values of the respective columns.
        /// </summary>
        [Input("inputFormatDefaultsForOmittedFields", required: true)]
        public Input<bool> InputFormatDefaultsForOmittedFields { get; set; } = null!;

        /// <summary>
        /// Enables or disables the insertion of JSON data with nested objects.
        /// </summary>
        [Input("inputFormatImportNestedJson", required: true)]
        public Input<bool> InputFormatImportNestedJson { get; set; } = null!;

        /// <summary>
        /// Enables or disables the initialization of NULL fields with default values, if data type of these fields is not nullable.
        /// </summary>
        [Input("inputFormatNullAsDefault", required: true)]
        public Input<bool> InputFormatNullAsDefault { get; set; } = null!;

        /// <summary>
        /// Enables or disables order-preserving parallel parsing of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats.
        /// </summary>
        [Input("inputFormatParallelParsing", required: true)]
        public Input<bool> InputFormatParallelParsing { get; set; } = null!;

        /// <summary>
        /// Enables or disables the full SQL parser if the fast stream parser can’t parse the data.
        /// </summary>
        [Input("inputFormatValuesInterpretExpressions", required: true)]
        public Input<bool> InputFormatValuesInterpretExpressions { get; set; } = null!;

        /// <summary>
        /// Enables or disables checking the column order when inserting data.
        /// </summary>
        [Input("inputFormatWithNamesUseHeader", required: true)]
        public Input<bool> InputFormatWithNamesUseHeader { get; set; } = null!;

        /// <summary>
        /// The setting sets the maximum number of retries for ClickHouse Keeper (or ZooKeeper) requests during insert into replicated MergeTree. Only Keeper requests which failed due to network error, Keeper session timeout, or request timeout are considered for retries.
        /// </summary>
        [Input("insertKeeperMaxRetries", required: true)]
        public Input<int> InsertKeeperMaxRetries { get; set; } = null!;

        /// <summary>
        /// Enables the insertion of default values instead of NULL into columns with not nullable data type. Default value: true.
        /// </summary>
        [Input("insertNullAsDefault", required: true)]
        public Input<bool> InsertNullAsDefault { get; set; } = null!;

        /// <summary>
        /// Enables the quorum writes.
        /// </summary>
        [Input("insertQuorum", required: true)]
        public Input<int> InsertQuorum { get; set; } = null!;

        /// <summary>
        /// Enables or disables parallelism for quorum INSERT queries.
        /// </summary>
        [Input("insertQuorumParallel", required: true)]
        public Input<bool> InsertQuorumParallel { get; set; } = null!;

        /// <summary>
        /// Write to a quorum timeout in milliseconds.
        /// </summary>
        [Input("insertQuorumTimeout", required: true)]
        public Input<int> InsertQuorumTimeout { get; set; } = null!;

        [Input("joinAlgorithms", required: true)]
        private InputList<string>? _joinAlgorithms;

        /// <summary>
        /// Specifies which JOIN algorithm is used. Possible values:
        /// * `hash` - hash join algorithm is used. The most generic implementation that supports all combinations of kind and strictness and multiple join keys that are combined with OR in the JOIN ON section.
        /// * `parallel_hash` - a variation of hash join that splits the data into buckets and builds several hash tables instead of one concurrently to speed up this process.
        /// * `partial_merge` - a variation of the sort-merge algorithm, where only the right table is fully sorted.
        /// * `direct` - this algorithm can be applied when the storage for the right table supports key-value requests.
        /// * `auto` - when set to auto, hash join is tried first, and the algorithm is switched on the fly to another algorithm if the memory limit is violated.
        /// * `full_sorting_merge` - sort-merge algorithm with full sorting joined tables before joining.
        /// * `prefer_partial_merge` - clickHouse always tries to use partial_merge join if possible, otherwise, it uses hash. Deprecated, same as partial_merge,hash.
        /// </summary>
        public InputList<string> JoinAlgorithms
        {
            get => _joinAlgorithms ?? (_joinAlgorithms = new InputList<string>());
            set => _joinAlgorithms = value;
        }

        /// <summary>
        /// Sets behavior on overflow in JOIN. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("joinOverflowMode", required: true)]
        public Input<string> JoinOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets the type of JOIN behavior. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting.
        /// </summary>
        [Input("joinUseNulls", required: true)]
        public Input<bool> JoinUseNulls { get; set; } = null!;

        /// <summary>
        /// Require aliases for subselects and table functions in FROM that more than one table is present.
        /// </summary>
        [Input("joinedSubqueryRequiresAlias", required: true)]
        public Input<bool> JoinedSubqueryRequiresAlias { get; set; } = null!;

        /// <summary>
        /// Specifies the algorithm of replicas selection that is used for distributed query processing, one of: random, nearest_hostname, in_order, first_or_random, round_robin. Default value: random.
        /// </summary>
        [Input("loadBalancing", required: true)]
        public Input<string> LoadBalancing { get; set; } = null!;

        /// <summary>
        /// Method of reading data from local filesystem. Possible values:
        /// * `read` - abort query execution, return an error.
        /// * `pread` - abort query execution, return an error.
        /// * `pread_threadpool` - stop query execution, return partial result. If the parameter is set to 0 (default), no hops is allowed.
        /// </summary>
        [Input("localFilesystemReadMethod", required: true)]
        public Input<string> LocalFilesystemReadMethod { get; set; } = null!;

        /// <summary>
        /// Setting up query threads logging. Query threads log into the system.query_thread_log table. This setting has effect only when log_queries is true. Queries’ threads run by ClickHouse with this setup are logged according to the rules in the query_thread_log server configuration parameter. Default value: `true`.
        /// </summary>
        [Input("logQueryThreads", required: true)]
        public Input<bool> LogQueryThreads { get; set; } = null!;

        /// <summary>
        /// Allows or restricts using the LowCardinality data type with the Native format.
        /// </summary>
        [Input("lowCardinalityAllowInNativeFormat", required: true)]
        public Input<bool> LowCardinalityAllowInNativeFormat { get; set; } = null!;

        /// <summary>
        /// Maximum abstract syntax tree depth.
        /// </summary>
        [Input("maxAstDepth", required: true)]
        public Input<int> MaxAstDepth { get; set; } = null!;

        /// <summary>
        /// Maximum abstract syntax tree elements.
        /// </summary>
        [Input("maxAstElements", required: true)]
        public Input<int> MaxAstElements { get; set; } = null!;

        /// <summary>
        /// A recommendation for what size of the block (in a count of rows) to load from tables.
        /// </summary>
        [Input("maxBlockSize", required: true)]
        public Input<int> MaxBlockSize { get; set; } = null!;

        /// <summary>
        /// Limit in bytes for using memory for GROUP BY before using swap on disk.
        /// </summary>
        [Input("maxBytesBeforeExternalGroupBy", required: true)]
        public Input<int> MaxBytesBeforeExternalGroupBy { get; set; } = null!;

        /// <summary>
        /// This setting is equivalent of the max_bytes_before_external_group_by setting, except for it is for sort operation (ORDER BY), not aggregation.
        /// </summary>
        [Input("maxBytesBeforeExternalSort", required: true)]
        public Input<int> MaxBytesBeforeExternalSort { get; set; } = null!;

        /// <summary>
        /// Limits the maximum size of a hash table in bytes (uncompressed data) when using DISTINCT.
        /// </summary>
        [Input("maxBytesInDistinct", required: true)]
        public Input<int> MaxBytesInDistinct { get; set; } = null!;

        /// <summary>
        /// Limit on maximum size of the hash table for JOIN, in bytes.
        /// </summary>
        [Input("maxBytesInJoin", required: true)]
        public Input<int> MaxBytesInJoin { get; set; } = null!;

        /// <summary>
        /// Limit on the number of bytes in the set resulting from the execution of the IN section.
        /// </summary>
        [Input("maxBytesInSet", required: true)]
        public Input<int> MaxBytesInSet { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be read from a table when running a query.
        /// </summary>
        [Input("maxBytesToRead", required: true)]
        public Input<int> MaxBytesToRead { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be read from a table for sorting.
        /// </summary>
        [Input("maxBytesToSort", required: true)]
        public Input<int> MaxBytesToSort { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
        /// </summary>
        [Input("maxBytesToTransfer", required: true)]
        public Input<int> MaxBytesToTransfer { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of columns that can be read from a table in a single query.
        /// </summary>
        [Input("maxColumnsToRead", required: true)]
        public Input<int> MaxColumnsToRead { get; set; } = null!;

        /// <summary>
        /// The maximum number of concurrent requests per user. Default value: 0 (no limit).
        /// </summary>
        [Input("maxConcurrentQueriesForUser", required: true)]
        public Input<int> MaxConcurrentQueriesForUser { get; set; } = null!;

        /// <summary>
        /// Limits the maximum query execution time in milliseconds.
        /// </summary>
        [Input("maxExecutionTime", required: true)]
        public Input<int> MaxExecutionTime { get; set; } = null!;

        /// <summary>
        /// Maximum abstract syntax tree depth after after expansion of aliases.
        /// </summary>
        [Input("maxExpandedAstElements", required: true)]
        public Input<int> MaxExpandedAstElements { get; set; } = null!;

        /// <summary>
        /// Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier.
        /// </summary>
        [Input("maxFinalThreads", required: true)]
        public Input<int> MaxFinalThreads { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of HTTP GET redirect hops for URL-engine tables.
        /// </summary>
        [Input("maxHttpGetRedirects", required: true)]
        public Input<int> MaxHttpGetRedirects { get; set; } = null!;

        /// <summary>
        /// The size of blocks (in a count of rows) to form for insertion into a table.
        /// </summary>
        [Input("maxInsertBlockSize", required: true)]
        public Input<int> MaxInsertBlockSize { get; set; } = null!;

        /// <summary>
        /// The maximum number of threads to execute the INSERT SELECT query. Default value: 0.
        /// </summary>
        [Input("maxInsertThreads", required: true)]
        public Input<int> MaxInsertThreads { get; set; } = null!;

        /// <summary>
        /// Limits the maximum memory usage (in bytes) for processing queries on a single server.
        /// </summary>
        [Input("maxMemoryUsage", required: true)]
        public Input<int> MaxMemoryUsage { get; set; } = null!;

        /// <summary>
        /// Limits the maximum memory usage (in bytes) for processing of user's queries on a single server.
        /// </summary>
        [Input("maxMemoryUsageForUser", required: true)]
        public Input<int> MaxMemoryUsageForUser { get; set; } = null!;

        /// <summary>
        /// Limits the speed of the data exchange over the network in bytes per second.
        /// </summary>
        [Input("maxNetworkBandwidth", required: true)]
        public Input<int> MaxNetworkBandwidth { get; set; } = null!;

        /// <summary>
        /// Limits the speed of the data exchange over the network in bytes per second.
        /// </summary>
        [Input("maxNetworkBandwidthForUser", required: true)]
        public Input<int> MaxNetworkBandwidthForUser { get; set; } = null!;

        /// <summary>
        /// Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size. Zero means unlimited.
        /// </summary>
        [Input("maxParserDepth", required: true)]
        public Input<int> MaxParserDepth { get; set; } = null!;

        /// <summary>
        /// The maximum part of a query that can be taken to RAM for parsing with the SQL parser.
        /// </summary>
        [Input("maxQuerySize", required: true)]
        public Input<int> MaxQuerySize { get; set; } = null!;

        /// <summary>
        /// The maximum size of the buffer to read from the filesystem.
        /// </summary>
        [Input("maxReadBufferSize", required: true)]
        public Input<int> MaxReadBufferSize { get; set; } = null!;

        /// <summary>
        /// Disables lagging replicas for distributed queries.
        /// </summary>
        [Input("maxReplicaDelayForDistributedQueries", required: true)]
        public Input<int> MaxReplicaDelayForDistributedQueries { get; set; } = null!;

        /// <summary>
        /// Limits the number of bytes in the result.
        /// </summary>
        [Input("maxResultBytes", required: true)]
        public Input<int> MaxResultBytes { get; set; } = null!;

        /// <summary>
        /// Limits the number of rows in the result.
        /// </summary>
        [Input("maxResultRows", required: true)]
        public Input<int> MaxResultRows { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of different rows when using DISTINCT.
        /// </summary>
        [Input("maxRowsInDistinct", required: true)]
        public Input<int> MaxRowsInDistinct { get; set; } = null!;

        /// <summary>
        /// Limit on maximum size of the hash table for JOIN, in rows.
        /// </summary>
        [Input("maxRowsInJoin", required: true)]
        public Input<int> MaxRowsInJoin { get; set; } = null!;

        /// <summary>
        /// Limit on the number of rows in the set resulting from the execution of the IN section.
        /// </summary>
        [Input("maxRowsInSet", required: true)]
        public Input<int> MaxRowsInSet { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of unique keys received from aggregation function.
        /// </summary>
        [Input("maxRowsToGroupBy", required: true)]
        public Input<int> MaxRowsToGroupBy { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of rows that can be read from a table when running a query.
        /// </summary>
        [Input("maxRowsToRead", required: true)]
        public Input<int> MaxRowsToRead { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of rows that can be read from a table for sorting.
        /// </summary>
        [Input("maxRowsToSort", required: true)]
        public Input<int> MaxRowsToSort { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of rows that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
        /// </summary>
        [Input("maxRowsToTransfer", required: true)]
        public Input<int> MaxRowsToTransfer { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns.
        /// </summary>
        [Input("maxTemporaryColumns", required: true)]
        public Input<int> MaxTemporaryColumns { get; set; } = null!;

        /// <summary>
        /// The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running queries. Zero means unlimited.
        /// </summary>
        [Input("maxTemporaryDataOnDiskSizeForQuery", required: true)]
        public Input<int> MaxTemporaryDataOnDiskSizeForQuery { get; set; } = null!;

        /// <summary>
        /// The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running user queries. Zero means unlimited.
        /// </summary>
        [Input("maxTemporaryDataOnDiskSizeForUser", required: true)]
        public Input<int> MaxTemporaryDataOnDiskSizeForUser { get; set; } = null!;

        /// <summary>
        /// Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, excluding constant columns.
        /// </summary>
        [Input("maxTemporaryNonConstColumns", required: true)]
        public Input<int> MaxTemporaryNonConstColumns { get; set; } = null!;

        /// <summary>
        /// The maximum number of query processing threads, excluding threads for retrieving data from remote servers.
        /// </summary>
        [Input("maxThreads", required: true)]
        public Input<int> MaxThreads { get; set; } = null!;

        /// <summary>
        /// It represents soft memory limit in case when hard limit is reached on user level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        /// </summary>
        [Input("memoryOvercommitRatioDenominator", required: true)]
        public Input<int> MemoryOvercommitRatioDenominator { get; set; } = null!;

        /// <summary>
        /// It represents soft memory limit in case when hard limit is reached on global level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        /// </summary>
        [Input("memoryOvercommitRatioDenominatorForUser", required: true)]
        public Input<int> MemoryOvercommitRatioDenominatorForUser { get; set; } = null!;

        /// <summary>
        /// Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation. Possible values: from 0 to 1. Default: 0.
        /// </summary>
        [Input("memoryProfilerSampleProbability", required: true)]
        public Input<double> MemoryProfilerSampleProbability { get; set; } = null!;

        /// <summary>
        /// Memory profiler step (in bytes). If the next query step requires more memory than this parameter specifies, the memory profiler collects the allocating stack trace. Values lower than a few megabytes slow down query processing. Default value: 4194304 (4 MB). Zero means disabled memory profiler.
        /// </summary>
        [Input("memoryProfilerStep", required: true)]
        public Input<int> MemoryProfilerStep { get; set; } = null!;

        /// <summary>
        /// Maximum time thread will wait for memory to be freed in the case of memory overcommit on a user level. If the timeout is reached and memory is not freed, an exception is thrown.
        /// </summary>
        [Input("memoryUsageOvercommitMaxWaitMicroseconds", required: true)]
        public Input<int> MemoryUsageOvercommitMaxWaitMicroseconds { get; set; } = null!;

        /// <summary>
        /// If ClickHouse should read more than merge_tree_max_bytes_to_use_cache bytes in one query, it doesn’t use the cache of uncompressed blocks.
        /// </summary>
        [Input("mergeTreeMaxBytesToUseCache", required: true)]
        public Input<int> MergeTreeMaxBytesToUseCache { get; set; } = null!;

        /// <summary>
        /// If ClickHouse should read more than merge_tree_max_rows_to_use_cache rows in one query, it doesn’t use the cache of uncompressed blocks.
        /// </summary>
        [Input("mergeTreeMaxRowsToUseCache", required: true)]
        public Input<int> MergeTreeMaxRowsToUseCache { get; set; } = null!;

        /// <summary>
        /// If the number of bytes to read from one file of a MergeTree-engine table exceeds merge_tree_min_bytes_for_concurrent_read, then ClickHouse tries to concurrently read from this file in several threads.
        /// </summary>
        [Input("mergeTreeMinBytesForConcurrentRead", required: true)]
        public Input<int> MergeTreeMinBytesForConcurrentRead { get; set; } = null!;

        /// <summary>
        /// If the number of rows to be read from a file of a MergeTree table exceeds merge_tree_min_rows_for_concurrent_read then ClickHouse tries to perform a concurrent reading from this file on several threads.
        /// </summary>
        [Input("mergeTreeMinRowsForConcurrentRead", required: true)]
        public Input<int> MergeTreeMinRowsForConcurrentRead { get; set; } = null!;

        /// <summary>
        /// The minimum data volume required for using direct I/O access to the storage disk.
        /// </summary>
        [Input("minBytesToUseDirectIo", required: true)]
        public Input<int> MinBytesToUseDirectIo { get; set; } = null!;

        /// <summary>
        /// How many times to potentially use a compiled chunk of code before running compilation.
        /// </summary>
        [Input("minCountToCompile", required: true)]
        public Input<int> MinCountToCompile { get; set; } = null!;

        /// <summary>
        /// A query waits for expression compilation process to complete prior to continuing execution.
        /// </summary>
        [Input("minCountToCompileExpression", required: true)]
        public Input<int> MinCountToCompileExpression { get; set; } = null!;

        /// <summary>
        /// Minimal execution speed in rows per second.
        /// </summary>
        [Input("minExecutionSpeed", required: true)]
        public Input<int> MinExecutionSpeed { get; set; } = null!;

        /// <summary>
        /// Minimal execution speed in bytes per second.
        /// </summary>
        [Input("minExecutionSpeedBytes", required: true)]
        public Input<int> MinExecutionSpeedBytes { get; set; } = null!;

        /// <summary>
        /// Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query.
        /// </summary>
        [Input("minInsertBlockSizeBytes", required: true)]
        public Input<int> MinInsertBlockSizeBytes { get; set; } = null!;

        /// <summary>
        /// Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query.
        /// </summary>
        [Input("minInsertBlockSizeRows", required: true)]
        public Input<int> MinInsertBlockSizeRows { get; set; } = null!;

        /// <summary>
        /// If the value is true, integers appear in quotes when using JSON* Int64 and UInt64 formats (for compatibility with most JavaScript implementations); otherwise, integers are output without the quotes.
        /// </summary>
        [Input("outputFormatJsonQuote64bitIntegers", required: true)]
        public Input<bool> OutputFormatJsonQuote64bitIntegers { get; set; } = null!;

        /// <summary>
        /// Enables +nan, -nan, +inf, -inf outputs in JSON output format.
        /// </summary>
        [Input("outputFormatJsonQuoteDenormals", required: true)]
        public Input<bool> OutputFormatJsonQuoteDenormals { get; set; } = null!;

        /// <summary>
        /// Enables/disables preferable using the localhost replica when processing distributed queries. Default value: true.
        /// </summary>
        [Input("preferLocalhostReplica", required: true)]
        public Input<bool> PreferLocalhostReplica { get; set; } = null!;

        /// <summary>
        /// Query priority.
        /// </summary>
        [Input("priority", required: true)]
        public Input<int> Priority { get; set; } = null!;

        /// <summary>
        /// Quota accounting mode.
        /// </summary>
        [Input("quotaMode", required: true)]
        public Input<string> QuotaMode { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow while read. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("readOverflowMode", required: true)]
        public Input<string> ReadOverflowMode { get; set; } = null!;

        /// <summary>
        /// Restricts permissions for reading data, write data and change settings queries.
        /// </summary>
        [Input("readonly", required: true)]
        public Input<int> Readonly { get; set; } = null!;

        /// <summary>
        /// Receive timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("receiveTimeout", required: true)]
        public Input<int> ReceiveTimeout { get; set; } = null!;

        /// <summary>
        /// Method of reading data from remote filesystem, one of: `read`, `threadpool`.
        /// </summary>
        [Input("remoteFilesystemReadMethod", required: true)]
        public Input<string> RemoteFilesystemReadMethod { get; set; } = null!;

        /// <summary>
        /// For ALTER ... ATTACH|DETACH|DROP queries, you can use the replication_alter_partitions_sync setting to set up waiting.
        /// </summary>
        [Input("replicationAlterPartitionsSync", required: true)]
        public Input<int> ReplicationAlterPartitionsSync { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow in result. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("resultOverflowMode", required: true)]
        public Input<string> ResultOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables or disables sequential consistency for SELECT queries.
        /// </summary>
        [Input("selectSequentialConsistency", required: true)]
        public Input<bool> SelectSequentialConsistency { get; set; } = null!;

        /// <summary>
        /// Enables or disables `X-ClickHouse-Progress` HTTP response headers in clickhouse-server responses.
        /// </summary>
        [Input("sendProgressInHttpHeaders", required: true)]
        public Input<bool> SendProgressInHttpHeaders { get; set; } = null!;

        /// <summary>
        /// Send timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("sendTimeout", required: true)]
        public Input<int> SendTimeout { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow in the set resulting. Possible values:
        ///   * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("setOverflowMode", required: true)]
        public Input<string> SetOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables or disables silently skipping of unavailable shards.
        /// </summary>
        [Input("skipUnavailableShards", required: true)]
        public Input<bool> SkipUnavailableShards { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow while sort. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("sortOverflowMode", required: true)]
        public Input<string> SortOverflowMode { get; set; } = null!;

        /// <summary>
        /// Timeout (in seconds) between checks of execution speed. It is checked that execution speed is not less that specified in min_execution_speed parameter. Must be at least 1000.
        /// </summary>
        [Input("timeoutBeforeCheckingExecutionSpeed", required: true)]
        public Input<int> TimeoutBeforeCheckingExecutionSpeed { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("timeoutOverflowMode", required: true)]
        public Input<string> TimeoutOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("transferOverflowMode", required: true)]
        public Input<string> TransferOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables equality of NULL values for IN operator.
        /// </summary>
        [Input("transformNullIn", required: true)]
        public Input<bool> TransformNullIn { get; set; } = null!;

        /// <summary>
        /// Enables hedged requests logic for remote queries. It allows to establish many connections with different replicas for query. New connection is enabled in case existent connection(s) with replica(s) were not established within hedged_connection_timeout or no data was received within receive_data_timeout. Query uses the first connection which send non empty progress packet (or data packet, if allow_changing_replica_until_first_data_packet); other connections are cancelled. Queries with max_parallel_replicas &gt; 1 are supported. Default value: true.
        /// </summary>
        [Input("useHedgedRequests", required: true)]
        public Input<bool> UseHedgedRequests { get; set; } = null!;

        /// <summary>
        /// Whether to use a cache of uncompressed blocks.
        /// </summary>
        [Input("useUncompressedCache", required: true)]
        public Input<bool> UseUncompressedCache { get; set; } = null!;

        /// <summary>
        /// Enables waiting for processing of asynchronous insertion. If enabled, server returns OK only after the data is inserted.
        /// </summary>
        [Input("waitForAsyncInsert", required: true)]
        public Input<bool> WaitForAsyncInsert { get; set; } = null!;

        /// <summary>
        /// The timeout (in seconds) for waiting for processing of asynchronous insertion. Value must be at least 1000 (1 second).
        /// </summary>
        [Input("waitForAsyncInsertTimeout", required: true)]
        public Input<int> WaitForAsyncInsertTimeout { get; set; } = null!;

        public GetMdbClickhouseClusterUserSettingsInputArgs()
        {
        }
        public static new GetMdbClickhouseClusterUserSettingsInputArgs Empty => new GetMdbClickhouseClusterUserSettingsInputArgs();
    }
}
