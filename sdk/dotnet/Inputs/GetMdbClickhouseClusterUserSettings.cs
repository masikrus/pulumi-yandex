// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.Yandex.Inputs
{

    public sealed class GetMdbClickhouseClusterUserSettingsArgs : global::Pulumi.InvokeArgs
    {
        /// <summary>
        /// Include CORS headers in HTTP responses.
        /// </summary>
        [Input("addHttpCorsHeader", required: true)]
        public bool AddHttpCorsHeader { get; set; }

        /// <summary>
        /// Allows or denies DDL queries.
        /// </summary>
        [Input("allowDdl", required: true)]
        public bool AllowDdl { get; set; }

        /// <summary>
        /// Enables introspections functions for query profiling.
        /// </summary>
        [Input("allowIntrospectionFunctions", required: true)]
        public bool AllowIntrospectionFunctions { get; set; }

        /// <summary>
        /// Allows specifying LowCardinality modifier for types of small fixed size (8 or less) in CREATE TABLE statements. Enabling this may increase merge times and memory consumption.
        /// </summary>
        [Input("allowSuspiciousLowCardinalityTypes", required: true)]
        public bool AllowSuspiciousLowCardinalityTypes { get; set; }

        /// <summary>
        /// Enables legacy ClickHouse server behavior in ANY INNER|LEFT JOIN operations.
        /// </summary>
        [Input("anyJoinDistinctRightTableKeys", required: true)]
        public bool AnyJoinDistinctRightTableKeys { get; set; }

        /// <summary>
        /// Enables asynchronous inserts. Disabled by default.
        /// </summary>
        [Input("asyncInsert", required: true)]
        public bool AsyncInsert { get; set; }

        /// <summary>
        /// The maximum timeout in milliseconds since the first INSERT query before inserting collected data. If the parameter is set to 0, the timeout is disabled. Default value: 200.
        /// </summary>
        [Input("asyncInsertBusyTimeout", required: true)]
        public int AsyncInsertBusyTimeout { get; set; }

        /// <summary>
        /// The maximum size of the unparsed data in bytes collected per query before being inserted. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 100000.
        /// </summary>
        [Input("asyncInsertMaxDataSize", required: true)]
        public int AsyncInsertMaxDataSize { get; set; }

        /// <summary>
        /// The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the async_insert_busy_timeout with every INSERT query as long as async_insert_max_data_size is not exceeded.
        /// </summary>
        [Input("asyncInsertStaleTimeout", required: true)]
        public int AsyncInsertStaleTimeout { get; set; }

        /// <summary>
        /// The maximum number of threads for background data parsing and insertion. If the parameter is set to 0, asynchronous insertions are disabled. Default value: 16.
        /// </summary>
        [Input("asyncInsertThreads", required: true)]
        public int AsyncInsertThreads { get; set; }

        /// <summary>
        /// Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response. Default value: false.
        /// </summary>
        [Input("cancelHttpReadonlyQueriesOnClientClose", required: true)]
        public bool CancelHttpReadonlyQueriesOnClientClose { get; set; }

        /// <summary>
        /// Enable compilation of queries.
        /// </summary>
        [Input("compile", required: true)]
        public bool Compile { get; set; }

        /// <summary>
        /// Turn on expression compilation.
        /// </summary>
        [Input("compileExpressions", required: true)]
        public bool CompileExpressions { get; set; }

        /// <summary>
        /// Connect timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("connectTimeout", required: true)]
        public int ConnectTimeout { get; set; }

        /// <summary>
        /// The timeout in milliseconds for connecting to a remote server for a Distributed table engine, if the ‘shard’ and ‘replica’ sections are used in the cluster definition. If unsuccessful, several attempts are made to connect to various replicas. Default value: 50.
        /// </summary>
        [Input("connectTimeoutWithFailover", required: true)]
        public int ConnectTimeoutWithFailover { get; set; }

        /// <summary>
        /// Specifies which of the uniq* functions should be used to perform the COUNT(DISTINCT …) construction.
        /// </summary>
        [Input("countDistinctImplementation", required: true)]
        public string CountDistinctImplementation { get; set; } = null!;

        /// <summary>
        /// Allows choosing a parser of the text representation of date and time, one of: `best_effort`, `basic`, `best_effort_us`. Default value: `basic`. Cloud default value: `best_effort`.
        /// </summary>
        [Input("dateTimeInputFormat", required: true)]
        public string DateTimeInputFormat { get; set; } = null!;

        /// <summary>
        /// Allows choosing different output formats of the text representation of date and time, one of: `simple`, `iso`, `unix_timestamp`. Default value: `simple`.
        /// </summary>
        [Input("dateTimeOutputFormat", required: true)]
        public string DateTimeOutputFormat { get; set; } = null!;

        /// <summary>
        /// Enables or disables the deduplication check for materialized views that receive data from `Replicated` tables.
        /// </summary>
        [Input("deduplicateBlocksInDependentMaterializedViews", required: true)]
        public bool DeduplicateBlocksInDependentMaterializedViews { get; set; }

        /// <summary>
        /// Sets behavior on overflow when using DISTINCT. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("distinctOverflowMode", required: true)]
        public string DistinctOverflowMode { get; set; } = null!;

        /// <summary>
        /// Determine the behavior of distributed subqueries.
        /// </summary>
        [Input("distributedAggregationMemoryEfficient", required: true)]
        public bool DistributedAggregationMemoryEfficient { get; set; }

        /// <summary>
        /// Timeout for DDL queries, in milliseconds.
        /// </summary>
        [Input("distributedDdlTaskTimeout", required: true)]
        public int DistributedDdlTaskTimeout { get; set; }

        /// <summary>
        /// Changes the behavior of distributed subqueries.
        /// </summary>
        [Input("distributedProductMode", required: true)]
        public string DistributedProductMode { get; set; } = null!;

        /// <summary>
        /// Allows to return empty result.
        /// </summary>
        [Input("emptyResultForAggregationByEmptySet", required: true)]
        public bool EmptyResultForAggregationByEmptySet { get; set; }

        /// <summary>
        /// Enables or disables data compression in the response to an HTTP request.
        /// </summary>
        [Input("enableHttpCompression", required: true)]
        public bool EnableHttpCompression { get; set; }

        /// <summary>
        /// Forces a query to an out-of-date replica if updated data is not available.
        /// </summary>
        [Input("fallbackToStaleReplicasForDistributedQueries", required: true)]
        public bool FallbackToStaleReplicasForDistributedQueries { get; set; }

        /// <summary>
        /// Sets the data format of a nested columns.
        /// </summary>
        [Input("flattenNested", required: true)]
        public bool FlattenNested { get; set; }

        /// <summary>
        /// Disables query execution if the index can’t be used by date.
        /// </summary>
        [Input("forceIndexByDate", required: true)]
        public bool ForceIndexByDate { get; set; }

        /// <summary>
        /// Disables query execution if indexing by the primary key is not possible.
        /// </summary>
        [Input("forcePrimaryKey", required: true)]
        public bool ForcePrimaryKey { get; set; }

        /// <summary>
        /// Regular expression (for Regexp format).
        /// </summary>
        [Input("formatRegexp", required: true)]
        public string FormatRegexp { get; set; } = null!;

        /// <summary>
        /// Skip lines unmatched by regular expression.
        /// </summary>
        [Input("formatRegexpSkipUnmatched", required: true)]
        public bool FormatRegexpSkipUnmatched { get; set; }

        /// <summary>
        /// Sets behavior on overflow while GROUP BY operation. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// * `any` - perform approximate GROUP BY operation by continuing aggregation for the keys that got into the set, but don’t add new keys to the set.
        /// </summary>
        [Input("groupByOverflowMode", required: true)]
        public string GroupByOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets the threshold of the number of keys, after that the two-level aggregation should be used.
        /// </summary>
        [Input("groupByTwoLevelThreshold", required: true)]
        public int GroupByTwoLevelThreshold { get; set; }

        /// <summary>
        /// Sets the threshold of the number of bytes, after that the two-level aggregation should be used.
        /// </summary>
        [Input("groupByTwoLevelThresholdBytes", required: true)]
        public int GroupByTwoLevelThresholdBytes { get; set; }

        /// <summary>
        /// Connection timeout for establishing connection with replica for Hedged requests. Default value: 50 milliseconds.
        /// </summary>
        [Input("hedgedConnectionTimeoutMs", required: true)]
        public int HedgedConnectionTimeoutMs { get; set; }

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpConnectionTimeout", required: true)]
        public int HttpConnectionTimeout { get; set; }

        /// <summary>
        /// Sets minimal interval between notifications about request process in HTTP header X-ClickHouse-Progress.
        /// </summary>
        [Input("httpHeadersProgressInterval", required: true)]
        public int HttpHeadersProgressInterval { get; set; }

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpReceiveTimeout", required: true)]
        public int HttpReceiveTimeout { get; set; }

        /// <summary>
        /// Timeout for HTTP connection in milliseconds.
        /// </summary>
        [Input("httpSendTimeout", required: true)]
        public int HttpSendTimeout { get; set; }

        /// <summary>
        /// Timeout to close idle TCP connections after specified number of seconds. Default value: 3600 seconds.
        /// </summary>
        [Input("idleConnectionTimeout", required: true)]
        public int IdleConnectionTimeout { get; set; }

        /// <summary>
        /// When performing INSERT queries, replace omitted input column values with default values of the respective columns.
        /// </summary>
        [Input("inputFormatDefaultsForOmittedFields", required: true)]
        public bool InputFormatDefaultsForOmittedFields { get; set; }

        /// <summary>
        /// Enables or disables the insertion of JSON data with nested objects.
        /// </summary>
        [Input("inputFormatImportNestedJson", required: true)]
        public bool InputFormatImportNestedJson { get; set; }

        /// <summary>
        /// Enables or disables the initialization of NULL fields with default values, if data type of these fields is not nullable.
        /// </summary>
        [Input("inputFormatNullAsDefault", required: true)]
        public bool InputFormatNullAsDefault { get; set; }

        /// <summary>
        /// Enables or disables order-preserving parallel parsing of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats.
        /// </summary>
        [Input("inputFormatParallelParsing", required: true)]
        public bool InputFormatParallelParsing { get; set; }

        /// <summary>
        /// Enables or disables the full SQL parser if the fast stream parser can’t parse the data.
        /// </summary>
        [Input("inputFormatValuesInterpretExpressions", required: true)]
        public bool InputFormatValuesInterpretExpressions { get; set; }

        /// <summary>
        /// Enables or disables checking the column order when inserting data.
        /// </summary>
        [Input("inputFormatWithNamesUseHeader", required: true)]
        public bool InputFormatWithNamesUseHeader { get; set; }

        /// <summary>
        /// The setting sets the maximum number of retries for ClickHouse Keeper (or ZooKeeper) requests during insert into replicated MergeTree. Only Keeper requests which failed due to network error, Keeper session timeout, or request timeout are considered for retries.
        /// </summary>
        [Input("insertKeeperMaxRetries", required: true)]
        public int InsertKeeperMaxRetries { get; set; }

        /// <summary>
        /// Enables the insertion of default values instead of NULL into columns with not nullable data type. Default value: true.
        /// </summary>
        [Input("insertNullAsDefault", required: true)]
        public bool InsertNullAsDefault { get; set; }

        /// <summary>
        /// Enables the quorum writes.
        /// </summary>
        [Input("insertQuorum", required: true)]
        public int InsertQuorum { get; set; }

        /// <summary>
        /// Enables or disables parallelism for quorum INSERT queries.
        /// </summary>
        [Input("insertQuorumParallel", required: true)]
        public bool InsertQuorumParallel { get; set; }

        /// <summary>
        /// Write to a quorum timeout in milliseconds.
        /// </summary>
        [Input("insertQuorumTimeout", required: true)]
        public int InsertQuorumTimeout { get; set; }

        [Input("joinAlgorithms", required: true)]
        private List<string>? _joinAlgorithms;

        /// <summary>
        /// Specifies which JOIN algorithm is used. Possible values:
        /// * `hash` - hash join algorithm is used. The most generic implementation that supports all combinations of kind and strictness and multiple join keys that are combined with OR in the JOIN ON section.
        /// * `parallel_hash` - a variation of hash join that splits the data into buckets and builds several hash tables instead of one concurrently to speed up this process.
        /// * `partial_merge` - a variation of the sort-merge algorithm, where only the right table is fully sorted.
        /// * `direct` - this algorithm can be applied when the storage for the right table supports key-value requests.
        /// * `auto` - when set to auto, hash join is tried first, and the algorithm is switched on the fly to another algorithm if the memory limit is violated.
        /// * `full_sorting_merge` - sort-merge algorithm with full sorting joined tables before joining.
        /// * `prefer_partial_merge` - clickHouse always tries to use partial_merge join if possible, otherwise, it uses hash. Deprecated, same as partial_merge,hash.
        /// </summary>
        public List<string> JoinAlgorithms
        {
            get => _joinAlgorithms ?? (_joinAlgorithms = new List<string>());
            set => _joinAlgorithms = value;
        }

        /// <summary>
        /// Sets behavior on overflow in JOIN. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("joinOverflowMode", required: true)]
        public string JoinOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets the type of JOIN behavior. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting.
        /// </summary>
        [Input("joinUseNulls", required: true)]
        public bool JoinUseNulls { get; set; }

        /// <summary>
        /// Require aliases for subselects and table functions in FROM that more than one table is present.
        /// </summary>
        [Input("joinedSubqueryRequiresAlias", required: true)]
        public bool JoinedSubqueryRequiresAlias { get; set; }

        /// <summary>
        /// Specifies the algorithm of replicas selection that is used for distributed query processing, one of: random, nearest_hostname, in_order, first_or_random, round_robin. Default value: random.
        /// </summary>
        [Input("loadBalancing", required: true)]
        public string LoadBalancing { get; set; } = null!;

        /// <summary>
        /// Method of reading data from local filesystem. Possible values:
        /// * `read` - abort query execution, return an error.
        /// * `pread` - abort query execution, return an error.
        /// * `pread_threadpool` - stop query execution, return partial result. If the parameter is set to 0 (default), no hops is allowed.
        /// </summary>
        [Input("localFilesystemReadMethod", required: true)]
        public string LocalFilesystemReadMethod { get; set; } = null!;

        /// <summary>
        /// Setting up query threads logging. Query threads log into the system.query_thread_log table. This setting has effect only when log_queries is true. Queries’ threads run by ClickHouse with this setup are logged according to the rules in the query_thread_log server configuration parameter. Default value: `true`.
        /// </summary>
        [Input("logQueryThreads", required: true)]
        public bool LogQueryThreads { get; set; }

        /// <summary>
        /// Allows or restricts using the LowCardinality data type with the Native format.
        /// </summary>
        [Input("lowCardinalityAllowInNativeFormat", required: true)]
        public bool LowCardinalityAllowInNativeFormat { get; set; }

        /// <summary>
        /// Maximum abstract syntax tree depth.
        /// </summary>
        [Input("maxAstDepth", required: true)]
        public int MaxAstDepth { get; set; }

        /// <summary>
        /// Maximum abstract syntax tree elements.
        /// </summary>
        [Input("maxAstElements", required: true)]
        public int MaxAstElements { get; set; }

        /// <summary>
        /// A recommendation for what size of the block (in a count of rows) to load from tables.
        /// </summary>
        [Input("maxBlockSize", required: true)]
        public int MaxBlockSize { get; set; }

        /// <summary>
        /// Limit in bytes for using memory for GROUP BY before using swap on disk.
        /// </summary>
        [Input("maxBytesBeforeExternalGroupBy", required: true)]
        public int MaxBytesBeforeExternalGroupBy { get; set; }

        /// <summary>
        /// This setting is equivalent of the max_bytes_before_external_group_by setting, except for it is for sort operation (ORDER BY), not aggregation.
        /// </summary>
        [Input("maxBytesBeforeExternalSort", required: true)]
        public int MaxBytesBeforeExternalSort { get; set; }

        /// <summary>
        /// Limits the maximum size of a hash table in bytes (uncompressed data) when using DISTINCT.
        /// </summary>
        [Input("maxBytesInDistinct", required: true)]
        public int MaxBytesInDistinct { get; set; }

        /// <summary>
        /// Limit on maximum size of the hash table for JOIN, in bytes.
        /// </summary>
        [Input("maxBytesInJoin", required: true)]
        public int MaxBytesInJoin { get; set; }

        /// <summary>
        /// Limit on the number of bytes in the set resulting from the execution of the IN section.
        /// </summary>
        [Input("maxBytesInSet", required: true)]
        public int MaxBytesInSet { get; set; }

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be read from a table when running a query.
        /// </summary>
        [Input("maxBytesToRead", required: true)]
        public int MaxBytesToRead { get; set; }

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be read from a table for sorting.
        /// </summary>
        [Input("maxBytesToSort", required: true)]
        public int MaxBytesToSort { get; set; }

        /// <summary>
        /// Limits the maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
        /// </summary>
        [Input("maxBytesToTransfer", required: true)]
        public int MaxBytesToTransfer { get; set; }

        /// <summary>
        /// Limits the maximum number of columns that can be read from a table in a single query.
        /// </summary>
        [Input("maxColumnsToRead", required: true)]
        public int MaxColumnsToRead { get; set; }

        /// <summary>
        /// The maximum number of concurrent requests per user. Default value: 0 (no limit).
        /// </summary>
        [Input("maxConcurrentQueriesForUser", required: true)]
        public int MaxConcurrentQueriesForUser { get; set; }

        /// <summary>
        /// Limits the maximum query execution time in milliseconds.
        /// </summary>
        [Input("maxExecutionTime", required: true)]
        public int MaxExecutionTime { get; set; }

        /// <summary>
        /// Maximum abstract syntax tree depth after after expansion of aliases.
        /// </summary>
        [Input("maxExpandedAstElements", required: true)]
        public int MaxExpandedAstElements { get; set; }

        /// <summary>
        /// Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier.
        /// </summary>
        [Input("maxFinalThreads", required: true)]
        public int MaxFinalThreads { get; set; }

        /// <summary>
        /// Limits the maximum number of HTTP GET redirect hops for URL-engine tables.
        /// </summary>
        [Input("maxHttpGetRedirects", required: true)]
        public int MaxHttpGetRedirects { get; set; }

        /// <summary>
        /// The size of blocks (in a count of rows) to form for insertion into a table.
        /// </summary>
        [Input("maxInsertBlockSize", required: true)]
        public int MaxInsertBlockSize { get; set; }

        /// <summary>
        /// The maximum number of threads to execute the INSERT SELECT query. Default value: 0.
        /// </summary>
        [Input("maxInsertThreads", required: true)]
        public int MaxInsertThreads { get; set; }

        /// <summary>
        /// Limits the maximum memory usage (in bytes) for processing queries on a single server.
        /// </summary>
        [Input("maxMemoryUsage", required: true)]
        public int MaxMemoryUsage { get; set; }

        /// <summary>
        /// Limits the maximum memory usage (in bytes) for processing of user's queries on a single server.
        /// </summary>
        [Input("maxMemoryUsageForUser", required: true)]
        public int MaxMemoryUsageForUser { get; set; }

        /// <summary>
        /// Limits the speed of the data exchange over the network in bytes per second.
        /// </summary>
        [Input("maxNetworkBandwidth", required: true)]
        public int MaxNetworkBandwidth { get; set; }

        /// <summary>
        /// Limits the speed of the data exchange over the network in bytes per second.
        /// </summary>
        [Input("maxNetworkBandwidthForUser", required: true)]
        public int MaxNetworkBandwidthForUser { get; set; }

        /// <summary>
        /// Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size. Zero means unlimited.
        /// </summary>
        [Input("maxParserDepth", required: true)]
        public int MaxParserDepth { get; set; }

        /// <summary>
        /// The maximum part of a query that can be taken to RAM for parsing with the SQL parser.
        /// </summary>
        [Input("maxQuerySize", required: true)]
        public int MaxQuerySize { get; set; }

        /// <summary>
        /// The maximum size of the buffer to read from the filesystem.
        /// </summary>
        [Input("maxReadBufferSize", required: true)]
        public int MaxReadBufferSize { get; set; }

        /// <summary>
        /// Disables lagging replicas for distributed queries.
        /// </summary>
        [Input("maxReplicaDelayForDistributedQueries", required: true)]
        public int MaxReplicaDelayForDistributedQueries { get; set; }

        /// <summary>
        /// Limits the number of bytes in the result.
        /// </summary>
        [Input("maxResultBytes", required: true)]
        public int MaxResultBytes { get; set; }

        /// <summary>
        /// Limits the number of rows in the result.
        /// </summary>
        [Input("maxResultRows", required: true)]
        public int MaxResultRows { get; set; }

        /// <summary>
        /// Limits the maximum number of different rows when using DISTINCT.
        /// </summary>
        [Input("maxRowsInDistinct", required: true)]
        public int MaxRowsInDistinct { get; set; }

        /// <summary>
        /// Limit on maximum size of the hash table for JOIN, in rows.
        /// </summary>
        [Input("maxRowsInJoin", required: true)]
        public int MaxRowsInJoin { get; set; }

        /// <summary>
        /// Limit on the number of rows in the set resulting from the execution of the IN section.
        /// </summary>
        [Input("maxRowsInSet", required: true)]
        public int MaxRowsInSet { get; set; }

        /// <summary>
        /// Limits the maximum number of unique keys received from aggregation function.
        /// </summary>
        [Input("maxRowsToGroupBy", required: true)]
        public int MaxRowsToGroupBy { get; set; }

        /// <summary>
        /// Limits the maximum number of rows that can be read from a table when running a query.
        /// </summary>
        [Input("maxRowsToRead", required: true)]
        public int MaxRowsToRead { get; set; }

        /// <summary>
        /// Limits the maximum number of rows that can be read from a table for sorting.
        /// </summary>
        [Input("maxRowsToSort", required: true)]
        public int MaxRowsToSort { get; set; }

        /// <summary>
        /// Limits the maximum number of rows that can be passed to a remote server or saved in a temporary table when using GLOBAL IN.
        /// </summary>
        [Input("maxRowsToTransfer", required: true)]
        public int MaxRowsToTransfer { get; set; }

        /// <summary>
        /// Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns.
        /// </summary>
        [Input("maxTemporaryColumns", required: true)]
        public int MaxTemporaryColumns { get; set; }

        /// <summary>
        /// The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running queries. Zero means unlimited.
        /// </summary>
        [Input("maxTemporaryDataOnDiskSizeForQuery", required: true)]
        public int MaxTemporaryDataOnDiskSizeForQuery { get; set; }

        /// <summary>
        /// The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running user queries. Zero means unlimited.
        /// </summary>
        [Input("maxTemporaryDataOnDiskSizeForUser", required: true)]
        public int MaxTemporaryDataOnDiskSizeForUser { get; set; }

        /// <summary>
        /// Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, excluding constant columns.
        /// </summary>
        [Input("maxTemporaryNonConstColumns", required: true)]
        public int MaxTemporaryNonConstColumns { get; set; }

        /// <summary>
        /// The maximum number of query processing threads, excluding threads for retrieving data from remote servers.
        /// </summary>
        [Input("maxThreads", required: true)]
        public int MaxThreads { get; set; }

        /// <summary>
        /// It represents soft memory limit in case when hard limit is reached on user level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        /// </summary>
        [Input("memoryOvercommitRatioDenominator", required: true)]
        public int MemoryOvercommitRatioDenominator { get; set; }

        /// <summary>
        /// It represents soft memory limit in case when hard limit is reached on global level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        /// </summary>
        [Input("memoryOvercommitRatioDenominatorForUser", required: true)]
        public int MemoryOvercommitRatioDenominatorForUser { get; set; }

        /// <summary>
        /// Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation. Possible values: from 0 to 1. Default: 0.
        /// </summary>
        [Input("memoryProfilerSampleProbability", required: true)]
        public double MemoryProfilerSampleProbability { get; set; }

        /// <summary>
        /// Memory profiler step (in bytes). If the next query step requires more memory than this parameter specifies, the memory profiler collects the allocating stack trace. Values lower than a few megabytes slow down query processing. Default value: 4194304 (4 MB). Zero means disabled memory profiler.
        /// </summary>
        [Input("memoryProfilerStep", required: true)]
        public int MemoryProfilerStep { get; set; }

        /// <summary>
        /// Maximum time thread will wait for memory to be freed in the case of memory overcommit on a user level. If the timeout is reached and memory is not freed, an exception is thrown.
        /// </summary>
        [Input("memoryUsageOvercommitMaxWaitMicroseconds", required: true)]
        public int MemoryUsageOvercommitMaxWaitMicroseconds { get; set; }

        /// <summary>
        /// If ClickHouse should read more than merge_tree_max_bytes_to_use_cache bytes in one query, it doesn’t use the cache of uncompressed blocks.
        /// </summary>
        [Input("mergeTreeMaxBytesToUseCache", required: true)]
        public int MergeTreeMaxBytesToUseCache { get; set; }

        /// <summary>
        /// If ClickHouse should read more than merge_tree_max_rows_to_use_cache rows in one query, it doesn’t use the cache of uncompressed blocks.
        /// </summary>
        [Input("mergeTreeMaxRowsToUseCache", required: true)]
        public int MergeTreeMaxRowsToUseCache { get; set; }

        /// <summary>
        /// If the number of bytes to read from one file of a MergeTree-engine table exceeds merge_tree_min_bytes_for_concurrent_read, then ClickHouse tries to concurrently read from this file in several threads.
        /// </summary>
        [Input("mergeTreeMinBytesForConcurrentRead", required: true)]
        public int MergeTreeMinBytesForConcurrentRead { get; set; }

        /// <summary>
        /// If the number of rows to be read from a file of a MergeTree table exceeds merge_tree_min_rows_for_concurrent_read then ClickHouse tries to perform a concurrent reading from this file on several threads.
        /// </summary>
        [Input("mergeTreeMinRowsForConcurrentRead", required: true)]
        public int MergeTreeMinRowsForConcurrentRead { get; set; }

        /// <summary>
        /// The minimum data volume required for using direct I/O access to the storage disk.
        /// </summary>
        [Input("minBytesToUseDirectIo", required: true)]
        public int MinBytesToUseDirectIo { get; set; }

        /// <summary>
        /// How many times to potentially use a compiled chunk of code before running compilation.
        /// </summary>
        [Input("minCountToCompile", required: true)]
        public int MinCountToCompile { get; set; }

        /// <summary>
        /// A query waits for expression compilation process to complete prior to continuing execution.
        /// </summary>
        [Input("minCountToCompileExpression", required: true)]
        public int MinCountToCompileExpression { get; set; }

        /// <summary>
        /// Minimal execution speed in rows per second.
        /// </summary>
        [Input("minExecutionSpeed", required: true)]
        public int MinExecutionSpeed { get; set; }

        /// <summary>
        /// Minimal execution speed in bytes per second.
        /// </summary>
        [Input("minExecutionSpeedBytes", required: true)]
        public int MinExecutionSpeedBytes { get; set; }

        /// <summary>
        /// Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query.
        /// </summary>
        [Input("minInsertBlockSizeBytes", required: true)]
        public int MinInsertBlockSizeBytes { get; set; }

        /// <summary>
        /// Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query.
        /// </summary>
        [Input("minInsertBlockSizeRows", required: true)]
        public int MinInsertBlockSizeRows { get; set; }

        /// <summary>
        /// If the value is true, integers appear in quotes when using JSON* Int64 and UInt64 formats (for compatibility with most JavaScript implementations); otherwise, integers are output without the quotes.
        /// </summary>
        [Input("outputFormatJsonQuote64bitIntegers", required: true)]
        public bool OutputFormatJsonQuote64bitIntegers { get; set; }

        /// <summary>
        /// Enables +nan, -nan, +inf, -inf outputs in JSON output format.
        /// </summary>
        [Input("outputFormatJsonQuoteDenormals", required: true)]
        public bool OutputFormatJsonQuoteDenormals { get; set; }

        /// <summary>
        /// Enables/disables preferable using the localhost replica when processing distributed queries. Default value: true.
        /// </summary>
        [Input("preferLocalhostReplica", required: true)]
        public bool PreferLocalhostReplica { get; set; }

        /// <summary>
        /// Query priority.
        /// </summary>
        [Input("priority", required: true)]
        public int Priority { get; set; }

        /// <summary>
        /// Quota accounting mode.
        /// </summary>
        [Input("quotaMode", required: true)]
        public string QuotaMode { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow while read. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("readOverflowMode", required: true)]
        public string ReadOverflowMode { get; set; } = null!;

        /// <summary>
        /// Restricts permissions for reading data, write data and change settings queries.
        /// </summary>
        [Input("readonly", required: true)]
        public int Readonly { get; set; }

        /// <summary>
        /// Receive timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("receiveTimeout", required: true)]
        public int ReceiveTimeout { get; set; }

        /// <summary>
        /// Method of reading data from remote filesystem, one of: `read`, `threadpool`.
        /// </summary>
        [Input("remoteFilesystemReadMethod", required: true)]
        public string RemoteFilesystemReadMethod { get; set; } = null!;

        /// <summary>
        /// For ALTER ... ATTACH|DETACH|DROP queries, you can use the replication_alter_partitions_sync setting to set up waiting.
        /// </summary>
        [Input("replicationAlterPartitionsSync", required: true)]
        public int ReplicationAlterPartitionsSync { get; set; }

        /// <summary>
        /// Sets behavior on overflow in result. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("resultOverflowMode", required: true)]
        public string ResultOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables or disables sequential consistency for SELECT queries.
        /// </summary>
        [Input("selectSequentialConsistency", required: true)]
        public bool SelectSequentialConsistency { get; set; }

        /// <summary>
        /// Enables or disables `X-ClickHouse-Progress` HTTP response headers in clickhouse-server responses.
        /// </summary>
        [Input("sendProgressInHttpHeaders", required: true)]
        public bool SendProgressInHttpHeaders { get; set; }

        /// <summary>
        /// Send timeout in milliseconds on the socket used for communicating with the client.
        /// </summary>
        [Input("sendTimeout", required: true)]
        public int SendTimeout { get; set; }

        /// <summary>
        /// Sets behavior on overflow in the set resulting. Possible values:
        ///   * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("setOverflowMode", required: true)]
        public string SetOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables or disables silently skipping of unavailable shards.
        /// </summary>
        [Input("skipUnavailableShards", required: true)]
        public bool SkipUnavailableShards { get; set; }

        /// <summary>
        /// Sets behavior on overflow while sort. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("sortOverflowMode", required: true)]
        public string SortOverflowMode { get; set; } = null!;

        /// <summary>
        /// Timeout (in seconds) between checks of execution speed. It is checked that execution speed is not less that specified in min_execution_speed parameter. Must be at least 1000.
        /// </summary>
        [Input("timeoutBeforeCheckingExecutionSpeed", required: true)]
        public int TimeoutBeforeCheckingExecutionSpeed { get; set; }

        /// <summary>
        /// Sets behavior on overflow. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("timeoutOverflowMode", required: true)]
        public string TimeoutOverflowMode { get; set; } = null!;

        /// <summary>
        /// Sets behavior on overflow. Possible values:
        /// * `throw` - abort query execution, return an error.
        /// * `break` - stop query execution, return partial result.
        /// </summary>
        [Input("transferOverflowMode", required: true)]
        public string TransferOverflowMode { get; set; } = null!;

        /// <summary>
        /// Enables equality of NULL values for IN operator.
        /// </summary>
        [Input("transformNullIn", required: true)]
        public bool TransformNullIn { get; set; }

        /// <summary>
        /// Enables hedged requests logic for remote queries. It allows to establish many connections with different replicas for query. New connection is enabled in case existent connection(s) with replica(s) were not established within hedged_connection_timeout or no data was received within receive_data_timeout. Query uses the first connection which send non empty progress packet (or data packet, if allow_changing_replica_until_first_data_packet); other connections are cancelled. Queries with max_parallel_replicas &gt; 1 are supported. Default value: true.
        /// </summary>
        [Input("useHedgedRequests", required: true)]
        public bool UseHedgedRequests { get; set; }

        /// <summary>
        /// Whether to use a cache of uncompressed blocks.
        /// </summary>
        [Input("useUncompressedCache", required: true)]
        public bool UseUncompressedCache { get; set; }

        /// <summary>
        /// Enables waiting for processing of asynchronous insertion. If enabled, server returns OK only after the data is inserted.
        /// </summary>
        [Input("waitForAsyncInsert", required: true)]
        public bool WaitForAsyncInsert { get; set; }

        /// <summary>
        /// The timeout (in seconds) for waiting for processing of asynchronous insertion. Value must be at least 1000 (1 second).
        /// </summary>
        [Input("waitForAsyncInsertTimeout", required: true)]
        public int WaitForAsyncInsertTimeout { get; set; }

        public GetMdbClickhouseClusterUserSettingsArgs()
        {
        }
        public static new GetMdbClickhouseClusterUserSettingsArgs Empty => new GetMdbClickhouseClusterUserSettingsArgs();
    }
}
